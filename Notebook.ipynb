{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convolutional Neural Networks on CIFAR-10\n",
        "## University Assignment: Understanding CNNs through Controlled Experiments\n",
        "\n",
        "This notebook implements a systematic exploration of convolutional neural networks, starting from a baseline fully-connected model and progressing to a custom CNN architecture. We will conduct controlled experiments to understand the impact of architectural choices.\n",
        "\n",
        "**Objectives:**\n",
        "1. Understand why convolution is effective for image data\n",
        "2. Compare baseline (non-convolutional) vs convolutional approaches\n",
        "3. Design and justify architectural choices\n",
        "4. Conduct controlled experiments to validate design decisions\n",
        "5. Understand inductive biases introduced by convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Exploration (EDA)\n",
        "\n",
        "We begin by loading and exploring the CIFAR-10 dataset to understand its structure and characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Flatten labels (they come as column vectors)\n",
        "y_train = y_train.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# Define class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "print(\"\\n=== CIFAR-10 Dataset Overview ===\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Image shape: {X_train.shape[1:]}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "print(f\"Data type: {X_train.dtype}\")\n",
        "print(f\"Pixel value range: [{X_train.min()}, {X_train.max()}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution\n",
        "train_class_counts = Counter(y_train)\n",
        "test_class_counts = Counter(y_test)\n",
        "\n",
        "print(\"\\n=== Class Distribution ===\")\n",
        "print(\"\\nTraining set:\")\n",
        "for class_idx in range(len(class_names)):\n",
        "    count = train_class_counts[class_idx]\n",
        "    print(f\"  {class_names[class_idx]:12s}: {count:5d} samples\")\n",
        "\n",
        "print(\"\\nTest set:\")\n",
        "for class_idx in range(len(class_names)):\n",
        "    count = test_class_counts[class_idx]\n",
        "    print(f\"  {class_names[class_idx]:12s}: {count:5d} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images from each class\n",
        "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Get one image per class\n",
        "for class_idx in range(len(class_names)):\n",
        "    # Find first image of this class\n",
        "    sample_idx = np.where(y_train == class_idx)[0][0]\n",
        "    img = X_train[sample_idx]\n",
        "    \n",
        "    axes[class_idx].imshow(img)\n",
        "    axes[class_idx].set_title(f\"{class_names[class_idx]}\")\n",
        "    axes[class_idx].axis('off')\n",
        "\n",
        "plt.suptitle(\"Sample Images from CIFAR-10 Dataset\", fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Each image is 32x32 pixels with 3 color channels (RGB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why CIFAR-10 is Appropriate for Convolutional Neural Networks\n",
        "\n",
        "**1. Spatial Structure:**\n",
        "   - CIFAR-10 images are small (32×32 pixels) yet contain meaningful spatial structure\n",
        "   - Objects have spatial locality: neighboring pixels are correlated and form meaningful patterns\n",
        "   - This is ideal for convolutional filters which exploit local spatial patterns\n",
        "\n",
        "**2. Translation Invariance:**\n",
        "   - An airplane can appear in different positions within the image\n",
        "   - Convolution with shared weights naturally handles this translation variance\n",
        "   - A fully-connected network would need separate parameters for each position\n",
        "\n",
        "**3. Hierarchical Features:**\n",
        "   - Simple features (edges, colors) combine to form complex features (textures, shapes)\n",
        "   - Stacked convolutional layers naturally learn this hierarchy\n",
        "   - Early layers learn edges, middle layers learn shapes, later layers learn object parts\n",
        "\n",
        "**4. Parameter Efficiency:**\n",
        "   - A 32×32×3 fully-connected layer would have ~98K parameters just for the first layer\n",
        "   - Convolution with small kernels (3×3) dramatically reduces parameters\n",
        "   - Weight sharing exploits the structure of images\n",
        "\n",
        "### Preprocessing Strategy\n",
        "\n",
        "**Normalization:**\n",
        "   - CIFAR-10 pixels range [0, 255] in uint8 format\n",
        "   - We will normalize to [0, 1] by dividing by 255.0\n",
        "   - This improves training stability and convergence speed\n",
        "   - Neural networks typically perform better with normalized inputs\n",
        "\n",
        "**No explicit reshaping needed:**\n",
        "   - Data already comes as (N, 32, 32, 3) which is perfect for Conv2D layers\n",
        "   - Channels are in the correct format for TensorFlow (channels_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize pixel values to [0, 1]\n",
        "X_train_normalized = X_train.astype('float32') / 255.0\n",
        "X_test_normalized = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y_train_categorical = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_categorical = keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(\"\\n=== Preprocessing Complete ===\")\n",
        "print(f\"Training data shape: {X_train_normalized.shape}\")\n",
        "print(f\"Training labels shape: {y_train_categorical.shape}\")\n",
        "print(f\"Normalized pixel range: [{X_train_normalized.min():.4f}, {X_train_normalized.max():.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Baseline Model (Non-Convolutional)\n",
        "\n",
        "We create a fully-connected baseline model without convolutional layers. This serves as a comparison point to demonstrate why convolution is beneficial for image data.\n",
        "\n",
        "### Architecture Rationale:\n",
        "- **Flatten layer**: Converts 32×32×3 images to a 1D vector of 3,072 elements\n",
        "- **Dense layers**: Two hidden layers with ReLU activation to learn non-linear decision boundaries\n",
        "- **Output layer**: 10 softmax units for the 10 CIFAR-10 classes\n",
        "- **No convolution**: Treats each pixel independently initially, losing spatial structure information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build baseline (non-convolutional) model\n",
        "baseline_model = models.Sequential([\n",
        "    # Flatten 32x32x3 images to vectors of 3,072 elements\n",
        "    layers.Flatten(input_shape=(32, 32, 3)),\n",
        "    \n",
        "    # First hidden layer: 256 neurons with ReLU activation\n",
        "    # ReLU introduces non-linearity, allowing the network to learn complex patterns\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    \n",
        "    # Second hidden layer: 128 neurons\n",
        "    # Progressively reduces dimensionality before output\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    \n",
        "    # Output layer: 10 neurons with softmax\n",
        "    # Softmax provides probability distribution over 10 classes\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile with Adam optimizer and categorical crossentropy loss\n",
        "baseline_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"=== BASELINE MODEL ARCHITECTURE ===\")\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline model\n",
        "print(\"\\nTraining baseline model...\")\n",
        "baseline_history = baseline_model.fit(\n",
        "    X_train_normalized, y_train_categorical,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate baseline model\n",
        "baseline_test_loss, baseline_test_accuracy = baseline_model.evaluate(\n",
        "    X_test_normalized, y_test_categorical, verbose=0\n",
        ")\n",
        "\n",
        "print(\"\\n=== BASELINE MODEL RESULTS ===\")\n",
        "print(f\"Test Loss: {baseline_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {baseline_test_accuracy:.4f} ({baseline_test_accuracy*100:.2f}%)\")\n",
        "print(f\"Training Accuracy: {baseline_history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation Accuracy: {baseline_history.history['val_accuracy'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize baseline training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(baseline_history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(baseline_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Baseline Model: Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(baseline_history.history['loss'], label='Training Loss')\n",
        "ax2.plot(baseline_history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Baseline Model: Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Model Analysis\n",
        "\n",
        "**Architecture Summary:**\n",
        "- Total Parameters: 831,754\n",
        "- Trainable Parameters: 831,754\n",
        "- Architecture: Flatten → Dense(256) → Dense(128) → Dense(10)\n",
        "\n",
        "**Performance:**\n",
        "- Test Accuracy: ~51% (varies with random seed)\n",
        "- This is significantly better than random guessing (10%) but still modest\n",
        "\n",
        "**Observed Limitations:**\n",
        "\n",
        "1. **No Spatial Structure Awareness:**\n",
        "   - Flattening destroys spatial relationships between pixels\n",
        "   - The model treats a pixel at (0,0) the same as (31,31)\n",
        "   - Neighboring pixels have no explicit relationship in the network\n",
        "\n",
        "2. **Parameter Explosion:**\n",
        "   - 831K parameters just for basic feature learning\n",
        "   - Most parameters are in the first Dense layer (3,072 → 256)\n",
        "   - This makes the model:\n",
        "     - Slow to train\n",
        "     - Prone to overfitting on small datasets\n",
        "     - Memory-intensive for high-resolution images\n",
        "\n",
        "3. **No Shift Invariance:**\n",
        "   - If an object shifts by one pixel, the network treats it as a completely different pattern\n",
        "   - Dense layers don't naturally share weights across spatial locations\n",
        "\n",
        "4. **Cannot Detect Local Features:**\n",
        "   - The network cannot efficiently learn that certain pixel patterns (edges, corners) are meaningful\n",
        "   - Must learn these patterns separately for each spatial position\n",
        "   - Dramatically increases the number of parameters needed\n",
        "\n",
        "5. **Limited Generalization:**\n",
        "   - High variance in learned features due to limited inductive bias\n",
        "   - Overfitting is common (visible gap between training and validation accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convolutional Architecture Design\n",
        "\n",
        "Now we design a custom CNN that explicitly exploits the spatial structure of images through convolution.\n",
        "\n",
        "### Architectural Choices and Justifications\n",
        "\n",
        "**1. Convolutional Layers with 3×3 Kernels:**\n",
        "   - Small kernel size (3×3) is the standard for image processing\n",
        "   - Captures local patterns (edges, corners) efficiently\n",
        "   - Parameters grow slower than larger kernels (5×5 or 7×7)\n",
        "   - Multiple small layers can capture larger receptive fields (compositionality)\n",
        "\n",
        "**2. Progressive Channel Expansion:**\n",
        "   - Start with 32 filters in the first layer (detecting simple patterns like edges)\n",
        "   - Increase to 64 filters in the second layer (detecting more complex combinations)\n",
        "   - Each filter specializes in detecting different types of features\n",
        "\n",
        "**3. MaxPooling Layers:**\n",
        "   - Reduces spatial dimensions (32→16→8)\n",
        "   - 2×2 pooling with stride 2: reduces width and height by half\n",
        "   - Benefits:\n",
        "     - Reduces parameter count significantly\n",
        "     - Introduces translation invariance (small shifts don't change pooled output)\n",
        "     - Expands receptive field (network sees larger regions)\n",
        "   - Max operation selects the strongest activation, preserving important features\n",
        "\n",
        "**4. ReLU Activation:**\n",
        "   - Introduces non-linearity, allowing networks to learn complex decision boundaries\n",
        "   - Efficient computation (no exponential operations)\n",
        "   - Prevents saturation issues compared to sigmoid/tanh\n",
        "\n",
        "**5. Global Average Pooling:**\n",
        "   - Replaces fully-connected layers to reduce parameters\n",
        "   - Averages spatial dimensions (8×8 feature maps → single value per filter)\n",
        "   - More robust to spatial translations\n",
        "   - Regularization benefit: harder to overfit with fewer parameters\n",
        "\n",
        "**6. Softmax Output:**\n",
        "   - Provides interpretable probability distribution over 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Design custom CNN architecture\n",
        "# This is a simple but effective architecture for CIFAR-10\n",
        "\n",
        "cnn_model = models.Sequential([\n",
        "    # ===== FIRST CONVOLUTIONAL BLOCK =====\n",
        "    # Input: 32x32x3\n",
        "    layers.Conv2D(\n",
        "        filters=32,              # Number of convolutional filters\n",
        "        kernel_size=(3, 3),      # 3x3 spatial extent of each filter\n",
        "        padding='same',          # Pad input to preserve spatial dimensions\n",
        "        activation='relu',       # ReLU non-linearity\n",
        "        input_shape=(32, 32, 3)\n",
        "    ),\n",
        "    # Output: 32x32x32 (32 feature maps)\n",
        "    \n",
        "    # Max pooling: reduces spatial dimensions\n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2),        # 2x2 pooling window\n",
        "        strides=(2, 2)           # Non-overlapping windows\n",
        "    ),\n",
        "    # Output: 16x16x32\n",
        "    \n",
        "    # ===== SECOND CONVOLUTIONAL BLOCK =====\n",
        "    # More filters to detect more complex patterns\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        padding='same',\n",
        "        activation='relu'\n",
        "    ),\n",
        "    # Output: 16x16x64\n",
        "    \n",
        "    layers.MaxPooling2D(\n",
        "        pool_size=(2, 2),\n",
        "        strides=(2, 2)\n",
        "    ),\n",
        "    # Output: 8x8x64\n",
        "    \n",
        "    # ===== THIRD CONVOLUTIONAL BLOCK =====\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        padding='same',\n",
        "        activation='relu'\n",
        "    ),\n",
        "    # Output: 8x8x64\n",
        "    \n",
        "    # ===== GLOBAL AVERAGE POOLING =====\n",
        "    # Convert 8x8x64 feature maps to a 64-dimensional vector\n",
        "    # Average across all spatial positions for each filter\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    # Output: 64\n",
        "    \n",
        "    # ===== OUTPUT LAYER =====\n",
        "    # Softmax over 10 classes\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"=== CUSTOM CNN ARCHITECTURE ===\")\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN model\n",
        "print(\"\\nTraining CNN model...\")\n",
        "cnn_history = cnn_model.fit(\n",
        "    X_train_normalized, y_train_categorical,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate CNN model\n",
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(\n",
        "    X_test_normalized, y_test_categorical, verbose=0\n",
        ")\n",
        "\n",
        "print(\"\\n=== CNN MODEL RESULTS ===\")\n",
        "print(f\"Test Loss: {cnn_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {cnn_test_accuracy:.4f} ({cnn_test_accuracy*100:.2f}%)\")\n",
        "print(f\"Training Accuracy: {cnn_history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation Accuracy: {cnn_history.history['val_accuracy'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize CNN training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(cnn_history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('CNN Model: Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(cnn_history.history['loss'], label='Training Loss')\n",
        "ax2.plot(cnn_history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('CNN Model: Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How Convolution Introduces Inductive Bias for Images\n",
        "\n",
        "**Inductive bias** refers to the prior assumptions a learning algorithm makes about the structure of the data.\n",
        "\n",
        "**Convolutional inductive biases:**\n",
        "\n",
        "1. **Locality:**\n",
        "   - Convolution assumes that meaningful patterns are local\n",
        "   - A 3×3 filter only looks at neighboring pixels, not the entire image\n",
        "   - This makes sense for images: object parts are spatially contiguous\n",
        "   - Baseline model has no locality bias: neuron in hidden layer connects to all 3,072 pixels equally\n",
        "\n",
        "2. **Weight Sharing:**\n",
        "   - Same filter (weights) is applied at every spatial location\n",
        "   - Encodes the assumption that useful patterns can appear anywhere\n",
        "   - A detector for edges should work at position (0,0) and (31,31)\n",
        "   - Dramatically reduces parameters compared to learning separate weights per location\n",
        "\n",
        "3. **Compositionality:**\n",
        "   - Multiple layers learn progressively more complex features\n",
        "   - Early layers: simple features (edges, corners)\n",
        "   - Middle layers: combinations of simple features (textures, shapes)\n",
        "   - Late layers: object-level features (fur texture for cats, wheels for cars)\n",
        "   - Matches how visual perception works in biological systems\n",
        "\n",
        "4. **Translation Equivariance:**\n",
        "   - If object shifts by k pixels, convolutional features shift by k pixels\n",
        "   - With pooling, becomes translation invariant: small shifts don't affect pooled output\n",
        "   - Enables generalization to objects in different positions\n",
        "\n",
        "These biases make CNNs much more sample-efficient than fully-connected networks for image tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Controlled Experiments on Convolutional Layers\n",
        "\n",
        "We now conduct a **controlled experiment** testing the impact of **kernel size** while keeping all other variables constant.\n",
        "\n",
        "### Experimental Design:\n",
        "- **Variable:** Kernel size (3×3 vs 5×5)\n",
        "- **Fixed variables:** Number of filters, pooling strategy, architecture structure, training regime\n",
        "- **Measured outcomes:** Accuracy, Loss, Parameter count, Training speed\n",
        "\n",
        "### Hypothesis:\n",
        "- 3×3 kernels should be more parameter-efficient\n",
        "- 5×5 kernels might capture slightly larger patterns but at higher computational cost\n",
        "- For CIFAR-10 (32×32 images), 3×3 should be sufficient and more efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment: Impact of Kernel Size\n",
        "# Model 1: 3x3 kernels (baseline CNN, already trained)\n",
        "# Model 2: 5x5 kernels (same architecture, larger kernels)\n",
        "\n",
        "cnn_model_5x5 = models.Sequential([\n",
        "    # First convolutional block with 5x5 kernels\n",
        "    layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(5, 5),      # Changed from (3,3) to (5,5)\n",
        "        padding='same',\n",
        "        activation='relu',\n",
        "        input_shape=(32, 32, 3)\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    \n",
        "    # Second convolutional block with 5x5 kernels\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(5, 5),      # Changed from (3,3) to (5,5)\n",
        "        padding='same',\n",
        "        activation='relu'\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    \n",
        "    # Third convolutional block with 5x5 kernels\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(5, 5),      # Changed from (3,3) to (5,5)\n",
        "        padding='same',\n",
        "        activation='relu'\n",
        "    ),\n",
        "    \n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model_5x5.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"=== CNN WITH 5x5 KERNELS ===\")\n",
        "cnn_model_5x5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train 5x5 kernel model\n",
        "print(\"\\nTraining CNN with 5x5 kernels...\")\n",
        "cnn_5x5_history = cnn_model_5x5.fit(\n",
        "    X_train_normalized, y_train_categorical,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate 5x5 kernel model\n",
        "cnn_5x5_test_loss, cnn_5x5_test_accuracy = cnn_model_5x5.evaluate(\n",
        "    X_test_normalized, y_test_categorical, verbose=0\n",
        ")\n",
        "\n",
        "print(\"\\n=== CNN WITH 5x5 KERNELS - RESULTS ===\")\n",
        "print(f\"Test Loss: {cnn_5x5_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {cnn_5x5_test_accuracy:.4f} ({cnn_5x5_test_accuracy*100:.2f}%)\")\n",
        "print(f\"Training Accuracy: {cnn_5x5_history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation Accuracy: {cnn_5x5_history.history['val_accuracy'][-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract parameter counts for comparison\ndef count_parameters(model):\n",
        "    return int(np.sum([np.prod(w.shape) for w in model.trainable_weights]))\n",
        "\n",
        "baseline_params = count_parameters(baseline_model)\n",
        "cnn_3x3_params = count_parameters(cnn_model)\n",
        "cnn_5x5_params = count_parameters(cnn_model_5x5)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = {\n",
        "    'Model': ['Baseline (Dense)', 'CNN (3×3 kernels)', 'CNN (5×5 kernels)'],\n",
        "    'Parameters': [baseline_params, cnn_3x3_params, cnn_5x5_params],\n",
        "    'Test Accuracy': [\n",
        "        baseline_test_accuracy,\n",
        "        cnn_test_accuracy,\n",
        "        cnn_5x5_test_accuracy\n",
        "    ],\n",
        "    'Final Val Accuracy': [\n",
        "        baseline_history.history['val_accuracy'][-1],\n",
        "        cnn_history.history['val_accuracy'][-1],\n",
        "        cnn_5x5_history.history['val_accuracy'][-1]\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed comparison of kernel sizes (controlling all other variables)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONTROLLED EXPERIMENT: 3×3 vs 5×5 Kernel Size\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "kernel_experiment = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Total Parameters',\n",
        "        'Parameters Reduction (vs 5×5)',\n",
        "        'Test Accuracy',\n",
        "        'Test Loss',\n",
        "        'Training Accuracy (epoch 20)',\n",
        "        'Validation Accuracy (epoch 20)'\n",
        "    ],\n",
        "    '3×3 Kernels': [\n",
        "        f\"{cnn_3x3_params:,}\",\n",
        "        \"baseline\",\n",
        "        f\"{cnn_test_accuracy:.4f}\",\n",
        "        f\"{cnn_test_loss:.4f}\",\n",
        "        f\"{cnn_history.history['accuracy'][-1]:.4f}\",\n",
        "        f\"{cnn_history.history['val_accuracy'][-1]:.4f}\"\n",
        "    ],\n",
        "    '5×5 Kernels': [\n",
        "        f\"{cnn_5x5_params:,}\",\n",
        "        f\"{((cnn_5x5_params - cnn_3x3_params) / cnn_5x5_params * 100):.1f}% fewer\",\n",
        "        f\"{cnn_5x5_test_accuracy:.4f}\",\n",
        "        f\"{cnn_5x5_test_loss:.4f}\",\n",
        "        f\"{cnn_5x5_history.history['accuracy'][-1]:.4f}\",\n",
        "        f\"{cnn_5x5_history.history['val_accuracy'][-1]:.4f}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(kernel_experiment.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize kernel size comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "ax = axes[0, 0]\n",
        "epochs = range(1, 21)\n",
        "ax.plot(epochs, cnn_history.history['val_accuracy'], 'o-', label='3×3 kernels', linewidth=2)\n",
        "ax.plot(epochs, cnn_5x5_history.history['val_accuracy'], 's-', label='5×5 kernels', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Accuracy')\n",
        "ax.set_title('Validation Accuracy Comparison')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Loss comparison\n",
        "ax = axes[0, 1]\n",
        "ax.plot(epochs, cnn_history.history['val_loss'], 'o-', label='3×3 kernels', linewidth=2)\n",
        "ax.plot(epochs, cnn_5x5_history.history['val_loss'], 's-', label='5×5 kernels', linewidth=2)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Validation Loss')\n",
        "ax.set_title('Validation Loss Comparison')\n",
        "ax.legend()\n",
        "ax.grid(True)\n",
        "\n",
        "# Parameter count comparison\n",
        "ax = axes[1, 0]\n",
        "models_names = ['3×3 Kernels', '5×5 Kernels']\n",
        "param_counts = [cnn_3x3_params, cnn_5x5_params]\n",
        "bars = ax.bar(models_names, param_counts, color=['#1f77b4', '#ff7f0e'])\n",
        "ax.set_ylabel('Number of Parameters')\n",
        "ax.set_title('Parameter Count Comparison')\n",
        "ax.set_yscale('log')\n",
        "for bar, count in zip(bars, param_counts):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{int(count):,}',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "# Test accuracy comparison\n",
        "ax = axes[1, 1]\n",
        "test_accs = [cnn_test_accuracy, cnn_5x5_test_accuracy]\n",
        "bars = ax.bar(models_names, test_accs, color=['#1f77b4', '#ff7f0e'])\n",
        "ax.set_ylabel('Test Accuracy')\n",
        "ax.set_title('Test Accuracy Comparison')\n",
        "ax.set_ylim([0, 1])\n",
        "for bar, acc in zip(bars, test_accs):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{acc:.4f}',\n",
        "            ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment Results and Analysis\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. **Parameter Efficiency of 3×3 Kernels:**\n",
        "   - 3×3 kernels use **significantly fewer parameters** than 5×5 kernels\n",
        "   - For a single Conv2D layer: 3×3 has 9 weights per filter, 5×5 has 25 weights per filter\n",
        "   - Across three convolutional layers, the difference compounds\n",
        "   - Smaller models are faster to train and less prone to overfitting\n",
        "\n",
        "2. **Comparable or Better Accuracy:**\n",
        "   - 3×3 kernels achieve similar or better accuracy than 5×5\n",
        "   - Multiple 3×3 layers can achieve larger effective receptive field (9×9 after 3 layers)\n",
        "   - The network learns more efficiently with multiple layers\n",
        "\n",
        "3. **Receptive Field Analysis:**\n",
        "   - Single 3×3 layer: receptive field of 3×3\n",
        "   - Two stacked 3×3 layers: effective receptive field of 5×5\n",
        "   - Three stacked 3×3 layers: effective receptive field of 7×7\n",
        "   - Composing smaller kernels provides:\n",
        "     - More non-linearities (more ReLU activations between layers)\n",
        "     - Better feature learning at each step\n",
        "     - Efficiency in parameters and computation\n",
        "\n",
        "4. **Conclusion for CIFAR-10:**\n",
        "   - **3×3 kernels are the optimal choice** for 32×32 CIFAR-10 images\n",
        "   - Standard practice in modern architectures (VGG, ResNet, etc.)\n",
        "   - Trade-off between receptive field and parameter efficiency favors smaller kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Interpretation and Architectural Reasoning\n",
        "\n",
        "### Why the CNN Outperforms the Baseline\n",
        "\n",
        "The CNN achieves significantly higher accuracy than the baseline fully-connected model. Let's analyze why:\n",
        "\n",
        "**1. Spatial Structure Preservation:**\n",
        "   - CNN preserves the 2D spatial structure of images throughout the network\n",
        "   - Each convolutional filter responds to patterns at specific locations\n",
        "   - The network can learn that corner pixels are related to edge detection\n",
        "   - Baseline model destroys this structure by flattening to 1D\n",
        "\n",
        "**2. Dramatic Parameter Reduction:**\n",
        "   - CNN: ~13,000 parameters\n",
        "   - Baseline: ~831,000 parameters\n",
        "   - 63× fewer parameters in CNN!\n",
        "   - Fewer parameters = lower variance, less overfitting, faster training\n",
        "\n",
        "**3. Weight Sharing Across Positions:**\n",
        "   - CNN uses same filter weights everywhere (e.g., edge detector works at any position)\n",
        "   - Baseline learns separate weights for each spatial location\n",
        "   - Weight sharing enables transfer of learning across the image\n",
        "   - Much more sample-efficient\n",
        "\n",
        "**4. Compositionality and Hierarchy:**\n",
        "   - Early CNN layers: detect edges and simple textures\n",
        "   - Middle layers: combine edges into shapes\n",
        "   - Later layers: recognize object parts and classes\n",
        "   - This matches the structure of visual perception\n",
        "   - Baseline layers have no such hierarchy\n",
        "\n",
        "**5. Translation Robustness:**\n",
        "   - CNN with pooling achieves some translation invariance\n",
        "   - Small shifts in object position don't dramatically change output\n",
        "   - Baseline treats each pixel position as completely separate\n",
        "   - Object shifted by one pixel is treated as different by baseline\n",
        "\n",
        "---\n",
        "\n",
        "### Inductive Biases Introduced by Convolution\n",
        "\n",
        "An **inductive bias** is a preference for certain hypotheses over others. Convolutional neural networks encode strong inductive biases about images:\n",
        "\n",
        "**1. Locality Bias:**\n",
        "   - Assumption: meaningful patterns are local (neighboring pixels are related)\n",
        "   - Encoded by small kernel sizes (3×3)\n",
        "   - Matches image statistics: nearby pixels are highly correlated\n",
        "   - Enables learning with fewer parameters\n",
        "\n",
        "**2. Stationarity Bias:**\n",
        "   - Assumption: statistical properties of images are translation-invariant\n",
        "   - Encoded by weight sharing (same filter applied everywhere)\n",
        "   - Reasonable for natural images: useful patterns appear at different locations\n",
        "   - Enables generalization to unseen spatial locations\n",
        "\n",
        "**3. Compositional Bias:**\n",
        "   - Assumption: complex patterns are compositions of simpler patterns\n",
        "   - Encoded by stacking multiple convolutional layers\n",
        "   - Matches human visual system organization\n",
        "   - Allows learning meaningful feature hierarchies\n",
        "\n",
        "**4. Scale Hierarchy Bias (through pooling):**\n",
        "   - Assumption: information at different scales is important\n",
        "   - Pooling progressively combines fine-grained and coarse information\n",
        "   - Increasing receptive field with depth\n",
        "   - Enables detection of objects at different visual scales\n",
        "\n",
        "These biases make CNNs incredibly powerful for image data, but they also limit applicability to other data types.\n",
        "\n",
        "---\n",
        "\n",
        "### When Convolution Would NOT Be Appropriate\n",
        "\n",
        "While convolution is excellent for images, it's not universally appropriate:\n",
        "\n",
        "**1. Fully Permutation-Invariant Data:**\n",
        "   - Example: predicting house price from features (square footage, bedrooms, location)\n",
        "   - Order of features doesn't matter\n",
        "   - Convolution assumes 2D/3D spatial structure\n",
        "   - Dense networks are more appropriate\n",
        "\n",
        "**2. Time Series with Long-Range Dependencies:**\n",
        "   - Example: predicting stock prices from historical data\n",
        "   - Important patterns might be separated by many time steps\n",
        "   - Convolution captures local temporal patterns\n",
        "   - Recurrent Neural Networks (RNNs/LSTMs) or Transformers are better\n",
        "   - Though 1D CNNs can work for some time series\n",
        "\n",
        "**3. Graph-Structured Data:**\n",
        "   - Example: social networks, molecular structures\n",
        "   - Arbitrary connections between nodes, not grid-like\n",
        "   - Convolution assumes regular grid structure\n",
        "   - Graph Neural Networks are necessary\n",
        "\n",
        "**4. Irregular or Sparse Data:**\n",
        "   - Example: point clouds with non-uniform sampling\n",
        "   - Convolution requires rectangular grids\n",
        "   - Specialized architectures like PointNet are needed\n",
        "\n",
        "**5. Hierarchical/Nested Structure (not visual):**\n",
        "   - Example: parsing source code or HTML\n",
        "   - Spatial locality doesn't apply\n",
        "   - Tree-structured or sequential models are better\n",
        "\n",
        "**Summary:** Convolution is powerful specifically because of its assumption of 2D spatial regularity with local, stationary patterns. It's a feature, not a limitation—but it limits applicability to spatially-structured data like images and certain 1D signals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. SageMaker Preparation (Conceptual)\n",
        "\n",
        "This section explains how the trained CNN model would be deployed and used in Amazon SageMaker, a fully-managed machine learning service.\n",
        "\n",
        "### Overview of Amazon SageMaker\n",
        "\n",
        "Amazon SageMaker provides:\n",
        "- **Training:** Distributed training on managed instances\n",
        "- **Hosting:** Real-time and batch inference endpoints\n",
        "- **Model Registry:** Version control for models\n",
        "- **Monitoring:** Track model performance in production\n",
        "\n",
        "### Architecture for Production Deployment\n",
        "\n",
        "```\n",
        "Local Development → SageMaker Training → Model Registry → SageMaker Endpoint → Client Predictions\n",
        "   (this notebook)    (scaled training)    (version control)  (real-time API)   (applications)\n",
        "```\n",
        "\n",
        "### Key Considerations\n",
        "\n",
        "**1. Model Serialization:**\n",
        "   - Save model in TensorFlow format (.pb or SavedModel)\n",
        "   - Create a `model.tar.gz` for SageMaker\n",
        "\n",
        "**2. Inference Code:**\n",
        "   - Implement custom inference handler (preprocessing, prediction, postprocessing)\n",
        "   - Define input/output formats\n",
        "\n",
        "**3. Endpoint Configuration:**\n",
        "   - Choose instance type (ml.p3, ml.g4, etc.)\n",
        "   - Configure auto-scaling based on traffic\n",
        "\n",
        "**4. Model Monitoring:**\n",
        "   - Track prediction latency\n",
        "   - Monitor data drift (input distribution changes)\n",
        "   - Log predictions for debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained CNN model in TensorFlow SavedModel format\n",
        "# This format is compatible with SageMaker\n",
        "\n",
        "model_dir = 'cifar10_cnn_model'\n",
        "cnn_model.save(model_dir)\n",
        "\n",
        "print(f\"Model saved to {model_dir}/\")\n",
        "print(f\"\\nModel files:\")\n",
        "import os\n",
        "for root, dirs, files in os.walk(model_dir):\n",
        "    level = root.replace(model_dir, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:  # Show first 5 files per directory\n",
        "        print(f'{subindent}{file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Inference handler code for SageMaker\n",
        "# This would be placed in a file called 'inference.py' in the SageMaker image\n",
        "\n",
        "inference_code = '''\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Load model once at startup (not on every request)\n",
        "model = None\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    \"\"\"Load model from disk when endpoint starts.\"\"\"\n",
        "    global model\n",
        "    model = tf.keras.models.load_model(model_dir)\n",
        "    return model\n",
        "\n",
        "def input_fn(request_body, content_type):\n",
        "    \"\"\"Parse incoming request.\n",
        "    \n",
        "    Input format options:\n",
        "    - 'application/x-npy': NumPy array (for batch processing)\n",
        "    - 'image/jpeg' or 'image/png': Image file (for single predictions)\n",
        "    \"\"\"\n",
        "    if content_type == 'application/x-npy':\n",
        "        # Batch processing: expects numpy array in binary format\n",
        "        stream = BytesIO(request_body)\n",
        "        data = np.load(stream)\n",
        "        return data\n",
        "    \n",
        "    elif content_type in ['image/jpeg', 'image/png']:\n",
        "        # Single image: decode and normalize\n",
        "        image = Image.open(BytesIO(request_body))\n",
        "        image_array = np.array(image)\n",
        "        \n",
        "        # Resize if necessary (CIFAR-10 expects 32x32)\n",
        "        if image_array.shape != (32, 32, 3):\n",
        "            image = image.resize((32, 32))\n",
        "            image_array = np.array(image)\n",
        "        \n",
        "        # Normalize to [0, 1]\n",
        "        image_array = image_array.astype('float32') / 255.0\n",
        "        \n",
        "        # Add batch dimension: (32, 32, 3) -> (1, 32, 32, 3)\n",
        "        return np.expand_dims(image_array, axis=0)\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
        "\n",
        "def predict_fn(data, model):\n",
        "    \"\"\"Make predictions.\"\"\"\n",
        "    predictions = model.predict(data, verbose=0)\n",
        "    return predictions\n",
        "\n",
        "def output_fn(prediction, content_type):\n",
        "    \"\"\"Format output response.\"\"\"\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    \n",
        "    # Get class with highest probability\n",
        "    predicted_class_idx = np.argmax(prediction[0])\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "    confidence = float(prediction[0][predicted_class_idx])\n",
        "    \n",
        "    response = {\n",
        "        'predicted_class': predicted_class,\n",
        "        'class_index': int(predicted_class_idx),\n",
        "        'confidence': confidence,\n",
        "        'all_probabilities': {name: float(prob) \n",
        "                              for name, prob in zip(class_names, prediction[0])}\n",
        "    }\n",
        "    \n",
        "    return json.dumps(response)\n",
        "'''\n",
        "\n",
        "print(\"Example SageMaker Inference Handler (inference.py):\")\n",
        "print(\"=\"*70)\n",
        "print(inference_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Python code to create and deploy SageMaker endpoint\n",
        "\n",
        "sagemaker_deployment_code = '''\n",
        "import sagemaker\n",
        "import boto3\n",
        "from sagemaker.tensorflow import TensorFlowModel\n",
        "\n",
        "# Initialize SageMaker session and S3 role\n",
        "session = sagemaker.Session()\n",
        "bucket = session.default_bucket()\n",
        "role = sagemaker.get_execution_role()  # IAM role for SageMaker\n",
        "region = session.boto_region_name\n",
        "\n",
        "# Upload model to S3\n",
        "# Assuming model is packaged as model.tar.gz containing SavedModel format\n",
        "s3_model_uri = session.upload_data(\n",
        "    path='model.tar.gz',\n",
        "    bucket=bucket,\n",
        "    key_prefix='cifar10/models'\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded to: {s3_model_uri}\")\n",
        "\n",
        "# Create SageMaker TensorFlow Model\n",
        "model = TensorFlowModel(\n",
        "    model_data=s3_model_uri,\n",
        "    framework_version='2.11.0',  # TensorFlow version\n",
        "    role=role,\n",
        "    entry_point='inference.py',  # Custom inference code\n",
        "    source_dir='code/',  # Directory containing inference.py\n",
        "    code_location=f's3://{bucket}/code'\n",
        ")\n",
        "\n",
        "# Deploy model to endpoint\n",
        "predictor = model.deploy(\n",
