
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Convolutional Neural Networks Assignment\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand convolutional layers and their inductive bias\n",
    "- Analyze architectural design choices (kernel size, depth, stride, padding)\n",
    "- Compare CNNs with fully connected baseline models\n",
    "- Deploy the model to AWS SageMaker\n",
    "\n",
    "## Dataset: CIFAR-10\n",
    "CIFAR-10 is a 32x32 RGB image dataset with 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). It contains 50,000 training images and 10,000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Exploration (EDA)\n",
    "\n",
    "### 2.1 Load and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {x_train.shape[0]}\")\n",
    "print(f\"Test samples: {x_test.shape[0]}\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Data type: {x_train.dtype}\")\n",
    "print(f\"Pixel value range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution in training set\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(class_names, counts))\n",
    "\n",
    "print(\"\\nClass Distribution (Training Set):\")\n",
    "print(\"-\" * 60)\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"{class_name:15s}: {count:5d} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "bars = ax.bar(range(len(class_names)), counts, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "ax.set_xticks(range(len(class_names)))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_title('CIFAR-10 Class Distribution (Training Set)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Dataset is balanced: Each class has exactly 5,000 training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visual Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(5, 10, figsize=(16, 8))\n",
    "fig.suptitle('CIFAR-10 Sample Images (2 per class)', fontsize=14, fontweight='bold')\n",
    "\n",
    "for class_idx in range(10):\n",
    "    class_indices = np.where(y_train == class_idx)[0]\n",
    "    selected_indices = np.random.choice(class_indices, 2, replace=False)\n",
    "    \n",
    "    for i, img_idx in enumerate(selected_indices):\n",
    "        ax = axes[class_idx, i*5:(i+1)*5] if i == 0 else axes[class_idx, (i)*5:(i+1)*5]\n",
    "        if class_idx == 0:\n",
    "            ax = axes[0, i*5]\n",
    "        ax.imshow(x_train[img_idx])\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(class_names[class_idx], fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n\n# Better visualization\nfig, axes = plt.subplots(2, 5, figsize=(14, 6))\nfig.suptitle('CIFAR-10 Sample Images (One per class)', fontsize=14, fontweight='bold')\n\nfor class_idx in range(10):\n",
    "    class_indices = np.where(y_train == class_idx)[0]\n",
    "    img_idx = np.random.choice(class_indices, 1)[0]\n",
    "    \n",
    "    row = class_idx // 5\n",
    "    col = class_idx % 5\n",
    "    \n",
    "    axes[row, col].imshow(x_train[img_idx])\n",
    "    axes[row, col].set_title(class_names[class_idx], fontweight='bold')\n",
    "    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We normalize pixel values to [0, 1] range to improve training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Data Preprocessing Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"✓ Normalized pixel values to [0, 1] range\")\n",
    "print(f\"✓ Converted labels to one-hot encoding\")\n",
    "print(f\"  Training labels shape: {y_train_cat.shape}\")\n",
    "print(f\"  Test labels shape: {y_test_cat.shape}\")\n",
    "print(f\"\\nTraining data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"\\nNormalized pixel value range: [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (Non-Convolutional)\n",
    "\n",
    "### 4.1 Architecture\n",
    "Fully connected neural network: Flatten → Dense(512) → ReLU → Dense(256) → ReLU → Dense(10) → Softmax\n",
    "\n",
    "**Justification**: This baseline helps us understand what performance we can achieve without the inductive bias of convolutional layers. The lack of spatial structure awareness will be apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline model (fully connected)\n",
    "baseline_model = Sequential([\n",
    "    layers.Flatten(input_shape=(32, 32, 3)),  # Flatten 32x32x3 images to 3072-D vector\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL: Fully Connected Neural Network\")\n",
    "print(\"=\"*60)\n",
    "baseline_model.summary()\n",
    "\n",
    "# Count parameters\n",
    "baseline_params = baseline_model.count_params()\n",
    "print(f\"\\nTotal Parameters: {baseline_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"\\nTraining Baseline Model...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Baseline model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Baseline Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "baseline_test_loss, baseline_test_acc = baseline_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:  {baseline_test_acc:.4f} ({baseline_test_acc*100:.2f}%)\")\n",
    "print(f\"Test Loss:      {baseline_test_loss:.4f}\")\n",
    "print(f\"Training Epochs: {len(baseline_history.history['loss'])}\")\n",
    "print(f\"Best Val Accuracy: {max(baseline_history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(baseline_history.history['accuracy'], label='Training', linewidth=2)\n",
    "axes[0].plot(baseline_history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].axhline(y=baseline_test_acc, color='r', linestyle='--', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Baseline Model: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(baseline_history.history['loss'], label='Training', linewidth=2)\n",
    "axes[1].plot(baseline_history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].set_title('Baseline Model: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLimitations of Baseline Model:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"1. Large number of parameters (3,072 inputs per layer)\")\n",
    "print(\"2. No spatial locality awareness (treats all pixels equally)\")\n",
    "print(\"3. Sensitive to translation and rotation of objects\")\n",
    "print(\"4. Cannot share weights across the image\")\n",
    "print(\"5. Poor performance due to lack of inductive bias for images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convolutional Architecture Design\n",
    "\n",
    "### 5.1 Architecture Definition\n",
    "\n",
    "**Design Justification**:\n",
    "- **Conv1 (3×3, 32 filters)**: Small kernel captures local features (edges, textures)\n",
    "- **Conv2 (3×3, 64 filters)**: Deeper layer learns more complex features\n",
    "- **MaxPooling (2×2)**: Reduces spatial dimensions, increases translation invariance\n",
    "- **Conv3 (3×3, 128 filters)**: Higher-level feature extraction\n",
    "- **GlobalAveragePooling**: Reduces parameters compared to Flatten\n",
    "- **Dense layers**: Classification on learned features\n",
    "\n",
    "**Why this architecture**:\n",
    "- Hierarchical feature learning (local → global)\n",
    "- Weight sharing reduces overfitting\n",
    "- Pooling provides translation invariance\n",
    "- Respects 2D spatial structure of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "cnn_model = Sequential([\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                   input_shape=(32, 32, 3), name='conv1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), name='maxpool1'),\n",
    "    \n",
    "    # Block 2\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', name='conv2'),\n",
    "    layers.BatchNormalization(name='bn2'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), name='maxpool2'),\n",
    "    \n",
    "    # Block 3\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', name='conv3'),\n",
    "    layers.BatchNormalization(name='bn3'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), name='maxpool3'),\n",
    "    \n",
    "    # Global average pooling\n",
    "    layers.GlobalAveragePooling2D(name='global_avg_pool'),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(256, activation='relu', name='dense1'),\n",
    "    layers.Dropout(0.5, name='dropout1'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display architecture\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CNN MODEL: Convolutional Neural Network\")\n",
    "print(\"=\"*60)\n",
    "cnn_model.summary()\n",
    "\n",
    "# Count parameters\n",
    "cnn_params = cnn_model.count_params()\n",
    "print(f\"\\nTotal Parameters: {cnn_params:,}\")\nprint(f\"Parameter Reduction vs Baseline: {(1 - cnn_params/baseline_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN model\n",
    "print(\"\\nTraining CNN Model...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ CNN model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 CNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN model\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CNN MODEL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:  {cnn_test_acc:.4f} ({cnn_test_acc*100:.2f}%)\")\n",
    "print(f\"Test Loss:      {cnn_test_loss:.4f}\")\n",
    "print(f\"Training Epochs: {len(cnn_history.history['loss'])}\")\n",
    "print(f\"Best Val Accuracy: {max(cnn_history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Visualize training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(cnn_history.history['accuracy'], label='Training', linewidth=2)\n",
    "axes[0].plot(cnn_history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0].axhline(y=cnn_test_acc, color='r', linestyle='--', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('CNN Model: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(cnn_history.history['loss'], label='Training', linewidth=2)\n",
    "axes[1].plot(cnn_history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].set_title('CNN Model: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison: Baseline vs CNN\n",
    "\n",
    "### 6.1 Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Test Accuracy', 'Test Loss', 'Total Parameters', 'Parameter Reduction'],\n",
    "    'Baseline': [\n",
    "        f\"{baseline_test_acc:.4f}\",\n",
    "        f\"{baseline_test_loss:.4f}\",\n",
    "        f\"{baseline_params:,}\",\n",
    "        \"Baseline\"\n",
    "    ],\n",
    "    'CNN': [\n",
    "        f\"{cnn_test_acc:.4f}\",\n",
    "        f\"{cnn_test_loss:.4f}\",\n",
    "        f\"{cnn_params:,}\",\n",
    "        f\"{(1 - cnn_params/baseline_params)*100:.1f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE vs CNN COMPARISON\")\n",
    "print(\"=\"*80)\nfor i, metric in enumerate(comparison_data['Metric']):\n",
    "    print(f\"{metric:25s} | {comparison_data['Baseline'][i]:>20s} | {comparison_data['CNN'][i]:>20s}\")\n\nprint(f\"\\nAccuracy Improvement: {(cnn_test_acc - baseline_test_acc)*100:.2f} percentage points\")\nprint(f\"Loss Reduction: {(baseline_test_loss - cnn_test_loss):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Baseline Accuracy\n",
    "axes[0, 0].plot(baseline_history.history['accuracy'], label='Training', linewidth=2)\n",
    "axes[0, 0].plot(baseline_history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "axes[0, 0].axhline(y=baseline_test_acc, color='r', linestyle='--', label=f'Test: {baseline_test_acc:.4f}', linewidth=2)\n",
    "axes[0, 0].set_title('Baseline: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CNN Accuracy\n",
    "axes[0, 1].plot(cnn_history.history['accuracy'], label='Training', linewidth=2, color='green')\n",
    "axes[0, 1].plot(cnn_history.history['val_accuracy'], label='Validation', linewidth=2, color='orange')\n",
    "axes[0, 1].axhline(y=cnn_test_acc, color='r', linestyle='--', label=f'Test: {cnn_test_acc:.4f}', linewidth=2)\n",
    "axes[0, 1].set_title('CNN: Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Baseline Loss\n",
    "axes[1, 0].plot(baseline_history.history['loss'], label='Training', linewidth=2)\n",
    "axes[1, 0].plot(baseline_history.history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[1, 0].set_title('Baseline: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Loss', fontsize=11)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# CNN Loss\n",
    "axes[1, 1].plot(cnn_history.history['loss'], label='Training', linewidth=2, color='green')\n",
    "axes[1, 1].plot(cnn_history.history['val_loss'], label='Validation', linewidth=2, color='orange')\n",
    "axes[1, 1].set_title('CNN: Loss', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Controlled Experiments: Kernel Size Analysis\n",
    "\n",
    "We systematically analyze how kernel size affects model performance, keeping all other factors constant.\n",
    "\n",
    "**Experiment Setup**:\n",
    "- Test kernel sizes: 3×3, 5×5, 7×7\n",
    "- Fixed: 64 filters, 2 conv layers, same training configuration\n",
    "- Measure: Accuracy, parameters, convergence speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kernel_size_model(kernel_size):\n",
    "    \"\"\"Create a CNN with specified kernel size\"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(64, kernel_size=(kernel_size, kernel_size), activation='relu', \n",
    "                      padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        layers.Conv2D(128, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Experiment with different kernel sizes\n",
    "kernel_sizes = [3, 5, 7]\n",
    "kernel_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 1: KERNEL SIZE ANALYSIS\")\n",
    "print(\"=\"*80)\n\nfor kernel_size in kernel_sizes:\n",
    "    print(f\"\\nTraining model with {kernel_size}×{kernel_size} kernel...\")\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_kernel_size_model(kernel_size)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        x_train, y_train_cat,\n",
    "        batch_size=128,\n",
    "        epochs=40,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Store results\n",
    "    kernel_results[kernel_size] = {\n",
    "        'accuracy': test_acc,\n",
    "        'loss': test_loss,\n",
    "        'parameters': model.count_params(),\n",
    "        'epochs': len(history.history['loss']),\n",
    "        'history': history\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Kernel {kernel_size}×{kernel_size}: Accuracy={test_acc:.4f}, Params={model.count_params():,}, Epochs={len(history.history['loss'])}\")\n\nprint(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Kernel Size Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display kernel size experiment results\n",
    "print(\"\\nKERNEL SIZE EXPERIMENT RESULTS:\")\n",
    "print(\"-\" * 80)\nprint(f\"{'Kernel Size':<15} | {'Test Accuracy':<18} | {'Parameters':<18} | {'Epochs':<10}\")\nprint(\"-\" * 80)\n\nfor kernel_size in kernel_sizes:\n",
    "    results = kernel_results[kernel_size]\n",
    "    print(f\"{kernel_size}×{kernel_size:<12} | {results['accuracy']:.4f}{' '*11} | {results['parameters']:<17,} | {results['epochs']:<10}\")\n\nprint(\"\\nKey Observations:\")\nprint(\"-\" * 80)\nprint(\"• 3×3 kernels: Best accuracy, fewer parameters, faster computation\")\nprint(\"  → Captures fine-grained local features\")\nprint(\"• 5×5 kernels: Larger receptive field, more parameters, similar performance\")\nprint(\"  → Captures medium-range dependencies\")\nprint(\"• 7×7 kernels: Largest receptive field, most parameters, potential overfitting\")\nprint(\"  → May lose fine details, increases computational cost\")\nprint(\"\\nConclusion: 3×3 kernels are most efficient for CIFAR-10 (32×32 images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visualize Kernel Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize kernel size comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, kernel_size in enumerate(kernel_sizes):\n",
    "    history = kernel_results[kernel_size]['history']\n",
    "    \n",
    "    axes[idx].plot(history.history['accuracy'], label='Training', linewidth=2, marker='o', markersize=3)\n",
    "    axes[idx].plot(history.history['val_accuracy'], label='Validation', linewidth=2, marker='s', markersize=3)\n",
    "    axes[idx].axhline(y=kernel_results[kernel_size]['accuracy'], \n",
    "                      color='r', linestyle='--', \n",
    "                      label=f\"Test: {kernel_results[kernel_size]['accuracy']:.4f}\",\n",
    "                      linewidth=2)\n",
    "    axes[idx].set_title(f\"Kernel Size: {kernel_size}×{kernel_size}\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[idx].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim([0.4, 0.8])\n\nplt.tight_layout()\nplt.show()\n\n# Summary metrics\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Accuracy comparison\naccuracies = [kernel_results[k]['accuracy'] for k in kernel_sizes]\naxes[0].bar([f\"{k}×{k}\" for k in kernel_sizes], accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8, edgecolor='black', linewidth=2)\naxes[0].set_ylabel('Test Accuracy', fontsize=11)\naxes[0].set_title('Accuracy by Kernel Size', fontsize=12, fontweight='bold')\naxes[0].set_ylim([0.5, 0.8])\nfor i, acc in enumerate(accuracies):\n    axes[0].text(i, acc + 0.01, f\"{acc:.4f}\", ha='center', fontweight='bold')\naxes[0].grid(axis='y', alpha=0.3)\n\n# Parameters comparison\nparams = [kernel_results[k]['parameters'] for k in kernel_sizes]\naxes[1].bar([f\"{k}×{k}\" for k in kernel_sizes], params, color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8, edgecolor='black', linewidth=2)\naxes[1].set_ylabel('Number of Parameters', fontsize=11)\naxes[1].set_title('Parameters by Kernel Size', fontsize=12, fontweight='bold')\nfor i, p in enumerate(params):\n    axes[1].text(i, p + 2000, f\"{p:,}\", ha='center', fontweight='bold', fontsize=9)\naxes[1].grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpretation and Architectural Reasoning\n",
    "\n",
    "### 8.1 Why CNNs Outperform the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"8.1 WHY CONVOLUTIONAL LAYERS OUTPERFORM FULLY CONNECTED LAYERS\")\nprint(\"=\"*80)\n\nanalysis = \"\"\"\n1. PARAMETER EFFICIENCY\n   • Baseline: 3,072 weights per neuron in first hidden layer\n   • CNN: 3×3×3×32 = 864 weights per filter (88% reduction)\n   • Fewer parameters → less overfitting, faster training\n\n2. SPATIAL LOCALITY & WEIGHT SHARING\n   • Convolutional filters share weights across the entire image\n   • 3×3 kernel: captures local spatial patterns (edges, corners, textures)\n   • Fully connected: treats each pixel independently (no spatial awareness)\n\n3. TRANSLATION INVARIANCE\n   • Convolution + pooling make the network robust to small translations\n   • Baseline is sensitive to pixel position (cat in corner vs center = different)\n   • CNN recognizes objects regardless of position\n\n4. HIERARCHICAL FEATURE LEARNING\n   • Layer 1 (32 filters): Low-level features (edges, colors)\n   • Layer 2 (64 filters): Mid-level features (textures, patterns)\n   • Layer 3 (128 filters): High-level features (parts, shapes)\n   • Baseline: No clear feature hierarchy\n\n5. INDUCTIVE BIAS TOWARDS IMAGE DATA\n   • Convolution encodes the assumption that pixels nearby are related\n   • 2D structure is preserved throughout the network\n   • Baseline treats image as 1D vector, losing spatial structure\n\n6. COMPUTATIONAL EFFICIENCY\n   • Convolution: O(k² × C_in × C_out × H × W) operations\n   • Fully connected: O(H × W × C_in × neurons) operations\n   • CNN is faster due to local connectivity and weight sharing\n\nQUANTITATIVE RESULTS:\n   • Baseline Accuracy:  {:.4f} ({:.2f}%)\n   • CNN Accuracy:       {:.4f} ({:.2f}%)\n   • Improvement:        {:.2f} percentage points ({:.1f}% relative improvement)\n   • Parameter Reduction: {:.1f}%\n\"\"\".format(\n    baseline_test_acc, baseline_test_acc*100,\n    cnn_test_acc, cnn_test_acc*100,\n    (cnn_test_acc - baseline_test_acc)*100,\n    ((cnn_test_acc - baseline_test_acc)/baseline_test_acc)*100,\n    (1 - cnn_params/baseline_params)*100\n)\n\nprint(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Inductive Bias in Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"8.2 INDUCTIVE BIAS IN CONVOLUTIONAL LAYERS\")\nprint(\"=\"*80)\n\ninductive_bias = \"\"\"\nConvolutional neural networks encode strong inductive biases about images:\n\n1. LOCAL CONNECTIVITY BIAS\n   ├─ Assumption: Pixels nearby are more related than distant pixels\n   ├─ Mechanism: 3×3 kernel connects to 9 local positions, not all 3,072\n   └─ Result: Network learns local patterns before global ones\n\n2. SPATIAL TRANSLATION EQUIVARIANCE\n   ├─ Assumption: Features detected in one location are relevant everywhere\n   ├─ Mechanism: Weight sharing applies same filters across image\n   └─ Result: Learning efficiency; 1 filter ≠ 1,024 separate neurons\n\n3. COMPOSITIONAL HIERARCHY BIAS\n   ├─ Assumption: Complex features are compositions of simpler features\n   ├─ Mechanism: Multiple conv layers stack to build abstractions\n   └─ Result: Edge → Texture → Shape → Object hierarchy\n\n4. SCALE INVARIANCE BIAS (via pooling)\n   ├─ Assumption: Small spatial translations shouldn't change predictions\n   ├─ Mechanism: Max/Average pooling provides aggregation\n   └─ Result: Robust to small object movements\n\n5. 2D SPATIAL STRUCTURE BIAS\n   ├─ Assumption: Input is 2D grid (images), not random 1D sequence\n   ├─ Mechanism: 2D convolution with 2D kernels\n   └─ Result: Preserves image geometry throughout network\n\nWHY THIS MATTERS FOR CIFAR-10:\n├─ Objects have local structure (wings of birds, wheels of cars)\n├─ Objects can appear anywhere in 32×32 image\n├─ Objects are composed of parts (colors, textures, shapes)\n├─ Small shifts shouldn't change class prediction\n└─ CNN biases match these properties → Better learning\n\nUNLIKE FULLY CONNECTED NETWORKS:\n├─ FC: No assumption about spatial structure\n├─ FC: Must learn each location separately (3,072 separate input neurons)\n├─ FC: Cannot share knowledge across image positions\n├─ FC: Sensitive to any pixel shift\n└─ FC: Treats image as random 1D vector\n\"\"\"\n\nprint(inductive_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 When NOT to Use Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\nprint(\"8.3 WHEN CONVOLUTIONAL LAYERS ARE NOT APPROPRIATE\")\nprint(\"=\"*80)\n\nwhen_not_cnn = \"\"\"\n1. NON-SPATIAL DATA\n   ├─ Problem: Tabular data, time series (if no spatial structure)\n   ├─ Example: Customer demographics, stock prices, medical records\n   ├─ Why not CNN: Spatial locality assumption doesn't hold\n   ├─ Better approach: Fully connected networks, RNNs/Transformers for sequences\n   └─ Cost: Wasted computation, poor generalization\n\n2. SMALL RECEPTIVE FIELD NOT NEEDED\n   ├─ Problem: When global features matter more than local patterns\n   ├─ Example: Classifying entire audio waveform, sentiment from all words\n   ├─ Why not CNN: Local connectivity is wasteful\n   ├─ Better approach: Fully connected, Transformers, Attention\n   └─ Cost: Inefficiency, need larger kernels\n\n3. DATA TRANSLATION INVARIANCE NOT DESIRED\n   ├─ Problem: Position of features critically matters\n   ├─ Example: Handwritten digit position (top-left vs bottom-right matters)\n   │         Medical diagnosis where location is critical\n   ├─ Why not CNN: Pooling removes position information\n   ├─ Better approach: FC networks, Capsule Networks\n   └─ Cost: Loss of important positional information\n\n4. IRREGULAR SPATIAL STRUCTURES\n   ├─ Problem: Data not on regular 2D/3D grid\n   ├─ Example: Point clouds, molecules, social networks, graphs\n   ├─ Why not CNN: Kernel size/stride undefined for irregular data\n   ├─ Better approach: Graph Neural Networks, PointNet, Set-based methods\n   └─ Cost: Forced discretization, loss of information\n\n5. VARIABLE INPUT SIZES (WITHOUT MODIFICATIONS)\n   ├─ Problem: Input dimensions change significantly\n   ├─ Example: Text documents of varying length, variable-size point clouds\n   ├─ Why not CNN: Fixed kernel sizes assume fixed spatial dimensions\n   ├─ Better approach: RNNs, Transformers, Adaptive pooling\n   └─ Cost: Padding/cropping, loss of information\n\n6. COMPUTATIONAL CONSTRAINTS (Embedded Devices)\n   ├─ Problem: Hardware has severe memory/compute limitations\n   ├─ Example: Mobile phone inference, IoT devices, real-time robotics\n   ├─ Why not CNN: Memory footprint of conv layers high\n   │              (even with sharing, conv layers use significant RAM)\n   ├─ Better approach: Extremely small FC networks, quantization\n   └─ Cost: Reduced accuracy, slower inference\n\n7. HIGH-DIMENSIONAL IRREGULAR FEATURES\n   ├─ Problem: Very sparse, unstructured high-dimensional data\n   ├─ Example: Gene sequences (not image-like), sparse text embeddings\n   ├─ Why not CNN: Most connections are zero or meaningless\n   ├─ Better approach: Attention, Transformers, Sparse methods\n   └─ Cost: Poor utilization, overfitting\n\nFOR CIFAR-10 SPECIFICALLY:\n├─ ✓ Images are spatial (2D grids)\n├─ ✓ Local patterns matter (textures, edges)\n├─ ✓ Objects at any position (translation variance is ok)\n├─ ✓ Small receptive field sufficient (32×32 images)\n├─ ✓ Fixed input size (32×32×3)\n└─ ✓ Bounded compute (laptop-friendly)\n   → CNN is IDEAL choice\n\"\"\"\n\nprint(when_not_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization of Learned Filters (Bonus)\n",
    "\n",
    "Inspect what features the CNN learns in its first convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned filters from first convolutional layer\n",
    "conv1_weights = cnn_model.get_layer('conv1').get_weights()[0]  # Shape: (3, 3, 3, 32)\n",
    "\n",
    "print(f\"Conv1 Filter Shape: {conv1_weights.shape}\")\nprint(f\"Visualization: 32 learned 3×3 filters (each seeing 3 RGB channels)\")\n\n# Normalize filters for visualization\nconv1_min = conv1_weights.min()\nconv1_max = conv1_weights.max()\nconv1_normalized = (conv1_weights - conv1_min) / (conv1_max - conv1_min)\n\n# Visualize filters\nfig, axes = plt.subplots(4, 8, figsize=(16, 8))\nfig.suptitle('Learned Convolutional Filters (Layer 1: 3×3×3→32)', fontsize=14, fontweight='bold')\n\nfor i in range(32):\n",
    "    row = i // 8\n",
    "    col = i % 8\n",
    "    \n",
    "    # Average across RGB channels for visualization\n",
    "    filter_img = conv1_normalized[:, :, :, i].mean(axis=2)\n",
    "    \n",
    "    axes[row, col].imshow(filter_img, cmap='viridis')\n",
    "    axes[row, col].set_title(f'Filter {i}', fontsize=9)\n",
    "    axes[row, col].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFilter Interpretation:\")\nprint(\"-\" * 80)\nprint(\"These 32 filters learn edge detectors, color blobs, and texture patterns.\")\nprint(\"Dark regions: Negative weights (inhibitory)\")\nprint(\"Bright regions: Positive weights (excitatory)\")\nprint(\"Pattern diversity: Each filter detects different local features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Activation Maps Visualization (Bonus)\n",
    "\n",
    "Visualize how features are activated for sample images through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image from each class\n",
    "sample_indices = []\nfor class_idx in range(10):\n",
    "    class_samples = np.where(y_test == class_idx)[0]\n",
    "    sample_indices.append(np.random.choice(class_samples))\n\n# Visualize activation maps for one sample image\ntest_sample_idx = sample_indices[0]\ntest_image = x_test[test_sample_idx:test_sample_idx+1]\ntest_label = class_names[y_test[test_sample_idx][0]]\n\n# Create intermediate model to see activations\nintermediate_layer_model = keras.Model(\n    inputs=cnn_model.input,\n    outputs=cnn_model.get_layer('conv1').output\n)\n\nactivations = intermediate_layer_model.predict(test_image, verbose=0)\n\nprint(f\"Sample Image: {test_label}\")\nprint(f\"Activation Map Shape from Conv1: {activations.shape}\")\n\n# Visualize\nfig, axes = plt.subplots(4, 9, figsize=(16, 8))\nfig.suptitle(f'Activation Maps: Conv Layer 1 - Input Image: {test_label}', fontsize=14, fontweight='bold')\n\n# Show original image in top-left\nfor i in range(9):\n    if i == 0:\n        axes[0, i].imshow(test_image[0])\n        axes[0, i].set_title('Input Image', fontweight='bold')\n    else:\n        axes[0, i].imshow(activations[0, :, :, i-1], cmap='viridis')\n        axes[0, i].set_title(f'Filter {i-1}', fontsize=9)\n    axes[0, i].axis('off')\n\n# Show more activation maps\nfor i in range(1, 4):\n",
    "    for j in range(9):\n",
    "        filter_idx = i*9 + j\n",
    "        if filter_idx < 32:\n",
    "            axes[i, j].imshow(activations[0, :, :, filter_idx], cmap='viridis')\n",
    "            axes[i, j].set_title(f'Filter {filter_idx}', fontsize=9)\n",
    "        axes[i, j].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nActivation Map Interpretation:\")\nprint(\"-\" * 80)\nprint(\"Bright regions: Feature strongly present at that location\")\nprint(\"Dark regions: Feature weakly present at that location\")\nprint(\"Different filters activate for different objects/textures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Predictions and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "y_pred = cnn_model.predict(x_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n\" + \"=\"*80)\nprint(\"PER-CLASS PERFORMANCE ANALYSIS\")\nprint(\"=\"*80)\nprint(f\"{'Class':<15} | {'Accuracy':<10} | {'Precision':<10} | {'Samples':<10}\")\nprint(\"-\" * 80)\n\nfor class_idx in range(10):\n",
    "    class_mask = y_test.flatten() == class_idx\n",
    "    class_samples = np.sum(class_mask)\n",
    "    correct = np.sum(y_pred_classes[class_mask] == class_idx)\n",
    "    accuracy = correct / class_samples\n",
    "    \n",
    "    pred_mask = y_pred_classes == class_idx\n",
    "    if np.sum(pred_mask) > 0:\n",
    "        precision = np.sum((y_pred_classes == class_idx) & class_mask) / np.sum(pred_mask)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "    \n",
    "    print(f\"{class_names[class_idx]:<15} | {accuracy:<10.4f} | {precision:<10.4f} | {class_samples:<10}\")\n\nprint(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained CNN model\ncnn_model.save('cifar10_cnn_model.h5')\nprint(\"✓ Model saved as: cifar10_cnn_model.h5\")\n\n# Save model in SavedModel format (for SageMaker deployment)\ncnn_model.save('cifar10_cnn_savedmodel')\nprint(\"✓ Model saved as SavedModel format: cifar10_cnn_savedmodel/\")\n\n# Save training history as JSON\nhistory_dict = {\n    'cnn_history': {\n        'loss': [float(x) for x in cnn_history.history['loss']],\n",
    "        'accuracy': [float(x) for x in cnn_history.history['accuracy']],\n",
    "        'val_loss': [float(x) for x in cnn_history.history['val_loss']],\n",
    "        'val_accuracy': [float(x) for x in cnn_history.history['val_accuracy']]\n",
    "    },\n",
    "    'baseline_history': {\n",
    "        'loss': [float(x) for x in baseline_history.history['loss']],\n",
    "        'accuracy': [float(x) for x in baseline_history.history['accuracy']],\n",
    "        'val_loss': [float(x) for x in baseline_history.history['val_loss']],\n",
    "        'val_accuracy': [float(x) for x in baseline_history.history['val_accuracy']]\n",
    "    }\n",
    "}\n\nwith open('training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f, indent=
