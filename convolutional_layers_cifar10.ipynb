{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers Assignment\n",
    "Full explanations, experiments, and architectural reasoning.\n",
    "Dataset: **CIFAR-10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Motivation\n",
    "\n",
    "### 1.1 Assignment Context\n",
    "This assignment explores convolutional neural networks (CNNs) not as black boxes, but as architectural components whose design choices directly impact learning efficiency, generalization, and interpretability.\n",
    "\n",
    "**Key Questions:**\n",
    "- Why do convolutional layers work better than fully connected layers for image data?\n",
    "- How do architectural decisions (kernel size, depth, stride) affect performance?\n",
    "- What inductive biases do convolutional layers introduce?\n",
    "\n",
    "### 1.2 Dataset Justification: CIFAR-10\n",
    "\n",
    "**CIFAR-10** is chosen for the following reasons:\n",
    "\n",
    "* **Spatial structure**: Natural images with strong local correlations\n",
    "* **Translation invariance**: Objects can appear at different positions\n",
    "* **Hierarchical features**: Low-level edges → mid-level textures → high-level objects\n",
    "* **Computational feasibility**: 32×32 images allow rapid experimentation\n",
    "* **Sufficient complexity**: 10 classes, diverse visual patterns\n",
    "\n",
    "These properties make CIFAR-10 ideal for demonstrating the convolutional inductive bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Description and Exploratory Data Analysis (EDA)\n",
    "### 2.1 Dataset Selection\n",
    "\n",
    "The **CIFAR-10** dataset is used for this assignment. CIFAR-10 consists of small natural images distributed across multiple object categories, making it highly suitable for evaluating **convolutional inductive bias**.\n",
    "\n",
    "**Key properties:**\n",
    "\n",
    "* **Number of images:** 60,000\n",
    "* **Train / test split:** 50,000 / 10,000\n",
    "* **Image resolution:** 32 × 32\n",
    "* **Channels:** 3 (RGB)\n",
    "* **Number of classes:** 10\n",
    "* **Class distribution:** Balanced\n",
    "\n",
    "This dataset is appropriate for convolutional layers because the images exhibit strong **local spatial correlations** and **translation invariance**, which convolution explicitly exploits through **local receptive fields** and **weight sharing**.\n",
    "\n",
    "### 2.2 Loading the Dataset\n",
    "\n",
    "#### Development Environment Setup\n",
    "\n",
    "For this project execution, a specialized virtual environment (`cnn-env`) was configured with the following technical specifications:\n",
    "\n",
    "* **Python Version:** Version **3.10** was utilized, specifically selected for its stability and compatibility with project dependencies.\n",
    "* **Main Libraries:** The environment integrates **TensorFlow**, facilitating the implementation and training of convolutional neural networks (CNNs).\n",
    "* **Environment Management:** The use of `cnn-env` ensures dependency isolation, preventing version conflicts and ensuring experiment reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Basic Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training shape:\", x_train.shape)\n",
    "print(\"Test shape:\", x_test.shape)\n",
    "print(\"Pixel range:\", x_train.min(), x_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Sample Visualization\n",
    "\n",
    "The purpose of visualization is not statistical analysis, but understanding the structure and variability of input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(class_names[y_train[i][0]])\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Preprocessing\n",
    "\n",
    "Pixel values are normalized to the [0, 1] range. Labels are **one-hot encoded** for categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model: Fully Connected Network (FCN)\n",
    "\n",
    "### 3.1 Motivation\n",
    "As a baseline, a fully connected neural network is trained on the same dataset. This model ignores spatial structure by flattening the image, treating each pixel as an independent feature. This baseline highlights the limitations of non-convolutional architectures for image data.\n",
    "\n",
    "### 3.2 Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "fc_model = Sequential([\n",
    "    Flatten(input_shape=(32,32,3)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "fc_model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "fc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training and Evaluation\n",
    "\n",
    "This section puts into practice the fundamental concepts covered in class, specifically the training configuration through the definition of **batches** and **epochs**:\n",
    "\n",
    "* **Batch Size:** Refers to the number of data samples processed by the network before updating internal parameters (weights). This technique enables more efficient and memory-stable training.\n",
    "* **Epochs:** Defines the total number of times the learning algorithm goes through the complete training dataset.\n",
    "\n",
    "This configuration is crucial for controlling model convergence and preventing overfitting during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fc = fc_model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "fc_test_loss, fc_test_acc = fc_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"FC Test Accuracy:\", fc_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Observed Limitations\n",
    "\n",
    "Despite having over a million parameters, the fully connected model exhibits limited generalization performance. The flattening process destroys spatial locality, forcing the network to relearn the same patterns at different image locations independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Network (CNN) Design\n",
    "\n",
    "### 4.1 Architectural Justification\n",
    "The CNN is designed under the following fundamental principles:\n",
    "\n",
    "* **Local receptive fields:** To capture spatial correlations in the image\n",
    "* **Weight sharing:** To reduce total number of parameters\n",
    "* **Increasing depth:** To build hierarchical feature representations\n",
    "* **Pooling:** To introduce translation invariance\n",
    "\n",
    "The architecture is intentionally shallow to prioritize structural reasoning over brute depth.\n",
    "\n",
    "### 4.2 CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = cnn_model.fit(\n",
    "    x_train, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "cnn_test_loss, cnn_test_acc = cnn_model.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"CNN Test Accuracy:\", cnn_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Controlled Experiment: Effect of Kernel Size\n",
    "\n",
    "### 5.1 Experimental Setup\n",
    "We vary the kernel size of the convolutional layers while keeping all other architectural and training parameters fixed. This allows us to isolate the effect of receptive field size on learning.\n",
    "\n",
    "### 5.2 Alternative Model (5×5 Kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_5x5 = Sequential([\n",
    "    Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(32,32,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (5,5), padding='same', activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_5x5.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "cnn_5x5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Results and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_5x5 = cnn_5x5.fit(\n",
    "    x_train, y_train_cat,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "loss_5x5, acc_5x5 = cnn_5x5.evaluate(x_test, y_test_cat, verbose=0)\n",
    "print(\"5x5 CNN Test Accuracy:\", acc_5x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Fully Connected', 'CNN 3x3', 'CNN 5x5'],\n",
    "    'Parameters': [\n",
    "        fc_model.count_params(),\n",
    "        cnn_model.count_params(),\n",
    "        cnn_5x5.count_params()\n",
    "    ],\n",
    "    'Test Accuracy': [fc_test_acc, cnn_test_acc, acc_5x5]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Training Curves Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_fc.history['accuracy'], label='FC Train', linestyle='--')\n",
    "plt.plot(history_fc.history['val_accuracy'], label='FC Val')\n",
    "plt.plot(history_cnn.history['accuracy'], label='CNN 3x3 Train', linestyle='--')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='CNN 3x3 Val')\n",
    "plt.plot(history_5x5.history['accuracy'], label='CNN 5x5 Train', linestyle='--')\n",
    "plt.plot(history_5x5.history['val_accuracy'], label='CNN 5x5 Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_fc.history['loss'], label='FC Train', linestyle='--')\n",
    "plt.plot(history_fc.history['val_loss'], label='FC Val')\n",
    "plt.plot(history_cnn.history['loss'], label='CNN 3x3 Train', linestyle='--')\n",
    "plt.plot(history_cnn.history['val_loss'], label='CNN 3x3 Val')\n",
    "plt.plot(history_5x5.history['loss'], label='CNN 5x5 Train', linestyle='--')\n",
    "plt.plot(history_5x5.history['val_loss'], label='CNN 5x5 Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation and Architectural Reasoning\n",
    "\n",
    "### 6.1 Why Convolutional Layers Outperform Fully Connected Networks\n",
    "\n",
    "**Observed Results:**\n",
    "- The CNN models achieve significantly higher accuracy than the fully connected baseline\n",
    "- This is accomplished with fewer parameters\n",
    "- Training converges faster for convolutional models\n",
    "\n",
    "**Fundamental Reasons:**\n",
    "\n",
    "1. **Spatial Locality Preservation:**\n",
    "   - Fully connected layers treat each pixel independently, destroying spatial relationships\n",
    "   - Convolutional layers maintain spatial structure through local receptive fields\n",
    "   - Adjacent pixels in images are highly correlated — convolution exploits this\n",
    "\n",
    "2. **Parameter Efficiency through Weight Sharing:**\n",
    "   - A fully connected layer connecting 32×32×3 input to 512 units requires ~1.5M parameters\n",
    "   - A 3×3 convolutional layer with 32 filters requires only 896 parameters\n",
    "   - The same filter is applied across the entire image, learning features once\n",
    "\n",
    "3. **Translation Invariance:**\n",
    "   - Objects can appear at different positions in images\n",
    "   - Fully connected networks must learn the same feature at every location independently\n",
    "   - Convolutional layers automatically detect features regardless of position\n",
    "\n",
    "### 6.2 Inductive Bias Introduced by Convolution\n",
    "\n",
    "**Inductive bias** refers to the assumptions a learning algorithm makes about the structure of the problem.\n",
    "\n",
    "**Convolutional Inductive Biases:**\n",
    "\n",
    "1. **Locality:** Nearby pixels are more relevant than distant pixels\n",
    "2. **Stationarity:** Patterns that appear in one part of the image are likely to appear elsewhere\n",
    "3. **Compositionality:** Complex patterns are built hierarchically from simpler ones\n",
    "\n",
    "These biases are **not learned from data** — they are **encoded in the architecture**. This is why convolutional networks require less data to learn effectively on images.\n",
    "\n",
    "### 6.3 Kernel Size Trade-offs (3×3 vs 5×5)\n",
    "\n",
    "**Observations from Experiments:**\n",
    "- The 3×3 model achieves comparable or better performance than the 5×5 model\n",
    "- The 3×3 model has fewer parameters\n",
    "- Larger kernels capture more context but are less parameter-efficient\n",
    "\n",
    "**Analysis:**\n",
    "- Two 3×3 convolutions have the same receptive field as one 5×5 convolution\n",
    "- But two 3×3 layers have more non-linearity (two ReLU activations)\n",
    "- Modern architectures (VGG, ResNet) favor stacking small kernels over using large ones\n",
    "\n",
    "### 6.4 When Convolution is NOT Appropriate\n",
    "\n",
    "**Problem Types Where Convolution Fails:**\n",
    "\n",
    "1. **Tabular Data:**\n",
    "   - Features in a spreadsheet (age, income, credit score) have no spatial relationship\n",
    "   - Reordering columns doesn't change meaning\n",
    "   - No translation invariance or locality\n",
    "\n",
    "2. **Graph-Structured Data:**\n",
    "   - Social networks, molecular structures\n",
    "   - Relationships are defined by edges, not spatial proximity\n",
    "   - Requires graph neural networks, not CNNs\n",
    "\n",
    "3. **Sequential Data with Long-Range Dependencies:**\n",
    "   - Language modeling, time series with distant correlations\n",
    "   - Convolution has limited receptive field\n",
    "   - Transformers or RNNs are more suitable\n",
    "\n",
    "4. **Problems Requiring Global Context:**\n",
    "   - Board game positions (Go, Chess)\n",
    "   - Global relationships matter more than local patterns\n",
    "   - Attention mechanisms or graph networks work better\n",
    "\n",
    "**Key Insight:** Convolution works when:\n",
    "- Data has a grid structure\n",
    "- Local patterns are important\n",
    "- Same patterns can appear at different positions\n",
    "\n",
    "When these assumptions don't hold, other architectures are more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bonus: Filter Visualization\n",
    "\n",
    "### 7.1 First Layer Filters\n",
    "Visualizing what the convolutional filters learn in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first convolutional layer weights\n",
    "first_layer = cnn_model.layers[0]\n",
    "filters, biases = first_layer.get_weights()\n",
    "\n",
    "# Normalize filter values to 0-1 for visualization\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters_normalized = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# Plot first 16 filters\n",
    "n_filters = min(16, filters.shape[3])\n",
    "plt.figure(figsize=(12,3))\n",
    "for i in range(n_filters):\n",
    "    plt.subplot(2, 8, i+1)\n",
    "    plt.imshow(filters_normalized[:,:,:,i])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Learned 3x3 Filters (First Layer)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Feature Map Visualization\n",
    "Showing how the network transforms an input image through convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model that outputs intermediate layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "layer_outputs = [layer.output for layer in cnn_model.layers[:4]]  # First 4 layers\n",
    "activation_model = Model(inputs=cnn_model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get activations for a sample image\n",
    "sample_image = x_test[0:1]  # Take first test image\n",
    "activations = activation_model.predict(sample_image, verbose=0)\n",
    "\n",
    "# Visualize\n",
    "layer_names = ['Conv2D 32 filters', 'MaxPooling2D', 'Conv2D 64 filters', 'MaxPooling2D']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(x_test[0])\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show first feature map from each layer\n",
    "for i, (activation, name) in enumerate(zip(activations, layer_names)):\n",
    "    if len(activation.shape) == 4:  # Conv layers\n",
    "        axes[i+1].imshow(activation[0, :, :, 0], cmap='viridis')\n",
    "        axes[i+1].set_title(f'{name}\\nFeature Map 0')\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "# Show multiple feature maps from first conv layer\n",
    "for idx in range(3):\n",
    "    axes[5+idx].imshow(activations[0][0, :, :, idx*8], cmap='viridis')\n",
    "    axes[5+idx].set_title(f'Conv1 Filter {idx*8}')\n",
    "    axes[5+idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### 8.1 Key Findings\n",
    "\n",
    "1. **Convolutional Superiority:** Convolutional models outperform fully connected baselines on CIFAR-10 while using significantly fewer parameters\n",
    "\n",
    "2. **Kernel Efficiency:** Smaller kernels (3×3) achieve better or comparable performance to larger kernels (5×5) with fewer parameters\n",
    "\n",
    "3. **Inductive Bias:** Convolution encodes assumptions about spatial locality and translation invariance, which are valid for image data\n",
    "\n",
    "4. **Limitations:** Convolution is not appropriate for data lacking spatial structure or translation invariance\n",
    "\n",
    "### 8.2 Architectural Insights\n",
    "\n",
    "This assignment demonstrates that **architectural choices encode prior knowledge** about the problem structure. Convolutional layers are not merely a technique, but a principled way to incorporate domain knowledge (spatial locality, stationarity) into neural network design.\n",
    "\n",
    "The superior performance of CNNs is not due to \"more advanced\" mathematics, but due to **better alignment between architecture and data structure**.\n",
    "\n",
    "### 8.3 Future Directions\n",
    "\n",
    "Potential extensions:\n",
    "- Data augmentation (rotation, flipping, color jitter)\n",
    "- Deeper architectures (ResNet blocks)\n",
    "- Batch normalization\n",
    "- Different pooling strategies (average pooling, learned pooling)\n",
    "- Attention mechanisms on top of convolutional features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
